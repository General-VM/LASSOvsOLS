---
title: "LASSO vs OLS"
author: "Victor Matyiku, Rebecca Ly"
date: "2025-03-06"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

# Code

## Generating Data

```{r}
generate_data <- function(p, n){
  beta0 <- rnorm(1)
  beta <- matrix(rnorm(p), nrow = p, ncol = 1)
  
  predictors <- matrix(
    rnorm(n*p), nrow = n, ncol = p,
    dimnames = list(rows = 1:n, cols = paste("X", 1:p, sep=""))
  )
  
  errors <- rnorm(n)
  observations <- data.frame(Y = beta0 + predictors %*% beta + errors)
  
  output <- as.data.frame(
    cbind(observations, predictors)
  )
  
  return(output)
}

#generated_data <- generate_data(10, 100)
#head(generated_data)
```

## OLS

```{r}
ols <- function(input_data){
  ols_model <- lm(Y ~ ., input_data)
  
  # extract only coefficients whose p value <= 0.5
  significant_coefs <- data.frame(Coefficients = summary(ols_model)$coef[summary(ols_model)$coef[,4] <= .05, 4])
  
  # transpose vector so calculations work properly later
  ols_coefs <- t(significant_coefs)
  significant_predictors <- colnames(ols_coefs)[colnames(ols_coefs) != "(Intercept)"]
  ols_coefs <- ols_coefs[, significant_predictors]

  # check if intercept is significant
  if (is.na(significant_coefs["(Intercept)",])) {
   ols_beta_0 <- 0
 } else {
   ols_beta_0 <- significant_coefs["(Intercept)",]
 }
  
  sig_predictors_matrix <- as.matrix(input_data[, significant_predictors])
  errors <- input_data$Y - ols_beta_0 - sig_predictors_matrix %*% ols_coefs

  # calculating squared error using euclidean norm
  output <- list(sq_error = (norm(errors, "e"))^2,
                 q = length(significant_predictors))
  
  return(output)
}

#ols(generated_data)
```

## LASSO

```{r}
library(glmnet)
lasso <- function(input_data){
  X <- as.matrix(input_data[colnames(input_data) != "Y"])
  Y <- as.matrix(input_data$Y)
  
  # LASSO with CV
  cv.fit <- cv.glmnet(X, Y, nfolds=10, family="gaussian") 

  # coefficients for LASSO model using optimal lambda
  lasso_coefs <- coef(cv.fit, s="lambda.min")
  
  no_intercept <- lasso_coefs@i[lasso_coefs@i != 0]

  #predict new coefficients using LASSO & optimal lambda
  prediction <- predict(cv.fit, newx = X, s="lambda.min")  

  # calculating squared error using euclidean norm
  output <- list(sq_error = (norm(Y-prediction, "e"))^2, 
                 q = length(no_intercept))

  return(output)
}

#lasso(generated_data)
```

## Repetition

```{r}
analysis_for <- function(p) {
  statistics <- list(
    ols_err = 1:100,
    lasso_err = 1:100,
    ols_q = 1:100,
    lasso_q = 1:100
  )
  
  for (i in 1:100) {
    generated_data <- generate_data(p, 100)
    ols_stats <- ols(generated_data)
    lasso_stats <- lasso(generated_data)
    
    statistics$ols_err[i] <- ols_stats$sq_error
    statistics$lasso_err[i] <- lasso_stats$sq_error
    statistics$ols_q[i] <- ols_stats$q
    statistics$lasso_q[i] <- lasso_stats$q
  }
  
  statistics$ols_mse <- sum(statistics$ols_err)/100
  statistics$lasso_mse <- sum(statistics$lasso_err)/100
  
  return(statistics)
}

#analysis_for(10)
```

## P values

```{r}
p_values <- c(2, 5, 10, 25, 50)

summary_df <- data.frame(
  p = p_values,
  OLS = p_values,
  LASSO = p_values #just to be consistent in size
)

for (i in 1:5) {
  p <- p_values[i]

  #getting statistics
  statistics <- analysis_for(p)
  ols_mse <- statistics$ols_mse
  lasso_mse <- statistics$lasso_mse
  
  #inputting into matrix
  summary_df$OLS[i] <- ols_mse
  summary_df$LASSO[i] <- lasso_mse
  
  print(paste0("Summary Statistics for p = ", p))
  writeLines("\nOLS Statistics\n")
  print(summary(statistics$ols_q))
  writeLines("\nLASSO Statistics\n")
  print(summary(statistics$lasso_q))
  writeLines("\n")
}

writeLines("Summary matrix of LASSO and OLS MSE\n")
kable(summary_df)
```
